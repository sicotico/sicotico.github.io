---
layout: post
title:  "Integraci贸n de Ollama con GitHub Copilot en Visual Studio Code: Gu铆a Completa"
date:   2025-09-15 12:00:00 -0500
categories: desarrollo ia github-copilot ollama
---

# Integraci贸n de Ollama con GitHub Copilot en Visual Studio Code: Gu铆a Completa

---

## Introducci贸n

La integraci贸n de **Ollama** con **GitHub Copilot** en **Visual Studio Code (VS Code)** marca un hito para los desarrolladores que buscan combinar la flexibilidad de modelos de inteligencia artificial locales con la potencia de un asistente de codificaci贸n en la nube. Desde **abril de 2025**, GitHub Copilot soporta oficialmente la integraci贸n con Ollama, permitiendo a los usuarios aprovechar modelos locales directamente desde el chat de Copilot en VS Code. Esto no solo mejora la privacidad y el control sobre los datos, sino que tambi茅n abre nuevas posibilidades para personalizar y optimizar el flujo de trabajo de desarrollo.

---

## 驴Qu茅 es Ollama y GitHub Copilot?

### Ollama
Ollama es una herramienta que permite ejecutar modelos de lenguaje grandes (LLM) de forma local, facilitando su integraci贸n en entornos de desarrollo. Es ideal para desarrolladores que prefieren mantener sus datos y procesos de IA dentro de su infraestructura local, ya sea por privacidad, rendimiento o costos.

### GitHub Copilot
GitHub Copilot es un asistente de codificaci贸n basado en IA desarrollado por GitHub y OpenAI. Se integra directamente en editores como VS Code, Visual Studio, y JetBrains, ofreciendo sugerencias de c贸digo en tiempo real, explicaciones, y asistencia contextual. Copilot utiliza modelos avanzados de IA para entender el contexto del c贸digo y generar sugerencias relevantes.

---

## 驴Desde cu谩ndo GitHub Copilot soporta Ollama?

La integraci贸n oficial de Ollama con GitHub Copilot se anunci贸 en **abril de 2025**. Esta actualizaci贸n permite a los usuarios seleccionar Ollama como proveedor de modelos dentro del chat de Copilot, lo que significa que ahora es posible utilizar modelos locales de Ollama directamente en el flujo de trabajo de desarrollo, sin depender exclusivamente de modelos en la nube.

---

## 驴C贸mo funciona la integraci贸n?

### Paso 1: Configuraci贸n de Ollama
1. **Instalar Ollama**: Aseg煤rate de tener Ollama instalado y ejecut谩ndose en tu m谩quina local o en un servidor accesible. Puedes descargar Ollama desde su [sitio oficial](https://ollama.ai/).
2. **Ejecutar un modelo**: Inicia un modelo de Ollama localmente. Por ejemplo:
   ```bash
   ollama serve
   ```
   Y luego carga el modelo que desees usar:
   ```bash
   ollama pull mistral
   ```

### Paso 2: Configuraci贸n en Visual Studio Code
1. **Instalar GitHub Copilot**: Aseg煤rate de tener la extensi贸n de GitHub Copilot instalada en VS Code. Si a煤n no la tienes, puedes instalarla desde el [Marketplace de VS Code](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot).
2. **Seleccionar Ollama como proveedor**:
   - Abre el chat de GitHub Copilot en VS Code.
   - Haz clic en el selector de modelos (model picker).
   - Ve a **Manage Models** (Administrar modelos).
   - Selecciona **Ollama** como proveedor.
   - Si Ollama est谩 ejecut谩ndose localmente, Copilot detectar谩 los modelos disponibles y te permitir谩 elegir uno.

### Paso 3: Configuraci贸n avanzada (Dev Containers)
Si trabajas en un **Dev Container**, necesitar谩s redirigir el tr谩fico de Ollama desde el contenedor a tu m谩quina host. Esto se puede lograr usando herramientas como `socat` o configurando el endpoint de Ollama en la configuraci贸n de VS Code Insiders:
```json
"github.copilot.chat.byok.ollamaEndpoint": "http://host.docker.internal:11434"
```
Esta configuraci贸n permite que el chat de Copilot se comunique con Ollama fuera del contenedor.

---

## Beneficios de la integraci贸n

- **Privacidad y control**: Al usar modelos locales, los datos sensibles nunca salen de tu entorno de desarrollo.
- **Personalizaci贸n**: Puedes elegir entre una amplia variedad de modelos de Ollama, optimizados para diferentes tareas y lenguajes de programaci贸n.
- **Flexibilidad**: Ideal para entornos con restricciones de conectividad o pol铆ticas de privacidad estrictas.
- **Rendimiento**: Reduce la latencia al evitar la dependencia de servicios en la nube.

---

## Casos de uso

1. **Desarrollo local seguro**: Ideal para proyectos que manejan datos sensibles o c贸digo propietario.
2. **Entornos sin conexi贸n**: Permite seguir utilizando asistencia de IA incluso sin acceso a internet.
3. **Experimentos con modelos**: Facilita probar diferentes modelos de Ollama sin cambiar de entorno.
4. **Educaci贸n y formaci贸n**: til para ense帽ar o aprender sobre IA y desarrollo de software en un entorno controlado.

---

## Limitaciones y consideraciones

- **Requisitos t茅cnicos**: Necesitas tener Ollama instalado y configurado correctamente, lo que puede requerir conocimientos t茅cnicos adicionales.
- **Rendimiento del hardware**: Ejecutar modelos locales puede consumir recursos significativos de CPU y RAM.
- **Compatibilidad**: Actualmente, esta funcionalidad est谩 disponible para usuarios de GitHub Copilot Free y Pro, pero es importante verificar las actualizaciones oficiales para cambios en los planes de suscripci贸n.

---

## Conclusi贸n

La integraci贸n de Ollama con GitHub Copilot en VS Code es un avance emocionante que combina lo mejor de ambos mundos: la potencia de la IA local y la conveniencia de un asistente de codificaci贸n en la nube. Con esta integraci贸n, los desarrolladores pueden personalizar su experiencia de desarrollo, mejorar la privacidad y optimizar su flujo de trabajo como nunca antes.

Si a煤n no has probado esta integraci贸n, 隆ahora es el momento perfecto para empezar! Configura Ollama, selecciona tus modelos favoritos y lleva tu productividad al siguiente nivel con GitHub Copilot.

---

**驴Ya has probado la integraci贸n de Ollama con GitHub Copilot? 隆Cu茅ntanos tu experiencia en los comentarios!**

---

### Recursos adicionales
- [Documentaci贸n oficial de GitHub Copilot](https://docs.github.com/copilot)
- [Gu铆a de Ollama](https://ollama.ai/)
- [Configuraci贸n avanzada en Dev Containers](https://www.returngis.net/2025/04/-como-usar-tus-modelos-de-ollama--en-github-copilot-chat/)
